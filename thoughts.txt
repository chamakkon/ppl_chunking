了解。以前の **coherence gain（チャンク全体を読んだときの尤度改善量）** を維持しつつ、**k-sent（直前 k 文までを文脈として許す）制約**を導入した DP アルゴリズムの仕様書をまとめます。
ポイントは「目的関数はそのまま」「文脈は最大 k 文に切る」「前計算で O(kN)・DP 遷移 O(1)」。

---

# 文チャンク分割アルゴリズム仕様書

## （Coherence Gain 保持 × k-sent 制約 × 区間DP）

## 1. 目的

文列 (S_1,\dots,S_N) をチャンク列 ({C_1,\dots,C_K}) に分割し、
**チャンクとして読むことによる LM の負の対数尤度（NLL）の改善量（coherence gain）** の総和を最大化する。
前計算で **各文に対する「直前 (m) 文文脈（(m\le k)）」の NLL** を用意し、**チャンク内 NLL を O(1)** で算出する。

---

## 2. 前提・記法

* 文列：(S_1,\dots,S_N)、各文のトークン列長は可変。
* k-sent 制約：チャンク内で文 (S_i) を評価する際に使える文脈は **チャンク先頭からの距離**に応じて **最大 (k) 文**まで。
* LM は token-level の logprob を返せる（温度0/greedy、同一トークナイザ）。
* NLL 定義：(\mathrm{NLL}(S \mid \text{ctx})= -\sum_{w\in S}\log P(w\mid \text{ctx}+ \text{prefix}(S)))。

---

## 3. スコア定義（coherence gain を維持）

### 3.1 文単体 NLL（文脈なし）

[
B_i = \mathrm{NLL}(S_i \mid \text{none})
]

### 3.2 k-sent 条件付き NLL テンソル

[
C_i^{(m)} = \mathrm{NLL}\big(S_i \mid S_{i-m},\dots,S_{i-1}\big),
\quad m=0,\dots,\min(k,,i-1)
]
（(m=0) のとき (C_i^{(0)}=B_i)）

### 3.3 チャンクの k-sent NLL

チャンク (C=S_{s..t}) を **先頭からの距離 (d=i-s)** で分け、距離 (d<k) はちょうど (m=d)、距離 (d\ge k) は (m=k) を用いる：
[
\mathrm{NLL}*k(s..t) ;=;
\sum*{i=s}^{\min(t,,s+k-1)} C_i^{(i-s)}
;;+;;
\sum_{i=s+k}^{t} C_i^{(k)}
]

### 3.4 チャンクの coherence gain（以前の目的関数のまま）

[
G_k(s..t) ;=; \sum_{i=s}^{t} B_i ;-; \mathrm{NLL}_k(s..t)
]
（※必要に応じて境界コスト (\alpha) を総和に (-\alpha\cdot #\text{boundaries}) として加算）

---

## 4. 前計算（効率化の肝）

### 4.1 (B_i) と (C_i^{(m)}) の前計算

* 各文単体：(B_i=C_i^{(0)})（N 回）
* 各文に対して (m=1..\min(k,i-1))：
  コンテキスト ((S_{i-m},\dots,S_{i-1})) を結合して (C_i^{(m)}) を取得
* 総 LM 呼び出し回数 ≈ **(O(kN))**

### 4.2 prefix 和で O(1) 区間評価

* (B) の prefix：(PB[j]=\sum_{i=1}^j B_i)
* (C^{(k)}) の prefix：(PC^{(k)}[j]=\sum_{i=1}^j C_i^{(k)})

すると
[
\sum_{i=s}^{t} B_i = PB[t]-PB[s-1]
]
[
\sum_{i=s+k}^{t} C_i^{(k)} = PC^{(k)}[t] - PC^{(k)}[s+k-1]
]
先頭から (k) 文以内の部分（高々 (k) 項）は配列参照で逐次足す（定数時間扱い）。

---

## 5. 区間DP（Viterbi 型）

### 5.1 状態

* (DP[j])：文 (1..j) を最適に分割したときの **coherence gain 総和（境界コスト込み）** の最大値
* (prev[j])：その最適解で最後のチャンクの開始位置 (s)

### 5.2 初期化

* (DP[0]=0)、(prev[0]=-1)

### 5.3 遷移

各 (j=1..N) に対して、すべての開始 (s\in[1..j]) を試す。
チャンク (S_{s..j}) のスコアは
[
\text{score}(s,j) ;=; DP[s-1] ;+; G_k(s..j) ;-; \alpha
]
（ただし (s=1) の先頭チャンクには境界コストを引かない設計でもよい。実装で統一）

**評価の内訳（すべて O(1)）**

* (\sum_{i=s}^j B_i = PB[j]-PB[s-1])
* (\mathrm{NLL}*k(s..j) = \underbrace{\sum*{i=s}^{\min(j,,s+k-1)} C_i^{(i-s)}}_{\text{高々 }k\text{ 項}}
  ;+; \big( PC^{(k)}[j]-PC^{(k)}[s+k-1] \big))（(s+k\le j) のとき）

最大の (\text{score}(s,j)) をとる (s) を選び、
[
DP[j]=\max_{s\le j}\text{score}(s,j),\quad prev[j]=\arg\max_s
]

### 5.4 復元

(j=N) から (s=prev[j]) をたどって区間を逆順に復元、最後に反転。

### 5.5 計算量

* 遷移評価：各 ((s,j)) が **O(1)** → DP 全体 **O(N²)**
* 前計算：LM 呼び出し **O(kN)**、prefix 構築 **O(N)**
* メモリ：(B_i)、(C_i^{(m)})（先頭側 (m<k) 用）、(PC^{(k)})、(PB)、(DP)、(prev)

---

## 6. 実装詳細・規約

### 6.1 プロンプト整形の固定

* 文脈結合：`S_{i-m} \n\n ... \n\n S_{i-1} <SEP> S_i` 等を**仕様で固定**
* BOS/EOS、改行、全角/半角、空白の扱いを固定（NLL が揺れるのを防止）
* **温度0 / greedy / 同一トークナイザ** を厳守

### 6.2 数値安定・スケーリング

* NLL はトークン長に比例→文長正規化（per-token NLL）を推奨

  * ( \hat{B}_i = B_i/\text{len}(S_i)), ( \hat{C}_i^{(m)} = C_i^{(m)}/\text{len}(S_i))
  * 同様に (PC^{(k)}), (PB) は正規化版で構築
* 境界コスト (\alpha) は **開発セットで P90〜P95 の (G_k(s..t)) スケール**に合わせてチューニング
* 必要なら温度パラメータ (\tau) により (G_k) を (G_k/\tau) でシャープ化

### 6.3 制約（任意）

* 最小/最大チャンク長（文/トークン）：
  遷移時に不適合区間をスキップ
* k の選定：2–3 が実用中庸、5 以上は LM 呼び出し増加と文脈過学習に注意

---

## 7. 疑似コード

```python
# 前計算
B = [0]*(N+1)                  # 1-indexed
C = [[None]*(k+1) for _ in range(N+1)]
for i in range(1, N+1):
    B[i] = NLL(S[i], ctx=None)            # per-tokenで正規化推奨
    C[i][0] = B[i]
    max_m = min(k, i-1)
    for m in range(1, max_m+1):
        ctx = concat(S[i-m:i])            # 固定整形
        C[i][m] = NLL(S[i], ctx=ctx)      # per-token

# prefix
PB = [0]*(N+1)
PCk = [0]*(N+1)
for i in range(1, N+1):
    PB[i] = PB[i-1] + B[i]
    # k-文脈が定義できる場合のみ
    if i >= 2:
        PCk[i] = PCk[i-1] + (C[i][min(k, i-1)] if (i-1) >= 1 else C[i][0])
    else:
        PCk[i] = 0

def NLL_k(s, t):
    # 先頭側（高々k項）
    end_head = min(t, s + k - 1)
    head = 0.0
    for i in range(s, end_head + 1):
        m = i - s
        head += C[i][m]
    # テール側（prefix差）
    tail = 0.0
    if s + k <= t:
        tail = PCk[t] - PCk[s + k - 1]
    return head + tail

def Gk(s, t):
    return (PB[t] - PB[s-1]) - NLL_k(s, t)

# DP
DP = [-1e30]*(N+1)
prev_idx = [-1]*(N+1)
DP[0] = 0.0
for j in range(1, N+1):
    best = -1e30
    best_s = -1
    for s in range(1, j+1):
        score = DP[s-1] + Gk(s, j) - (0.0 if s==1 else alpha)
        if score > best:
            best = score
            best_s = s
    DP[j] = best
    prev_idx[j] = best_s

# 復元
segments = []
j = N
while j > 0:
    s = prev_idx[j]
    segments.append((s, j))
    j = s - 1
segments.reverse()
```

---

## 8. 検証・チューニング手順

1. **再現性チェック**：固定乱数・同一整形で (B_i, C_i^{(m)}) の分布を保存
2. **α スイープ**：開発セットで分割数、平均長、目的タスク（RAG 検索 nDCG/QA EM/F1）を最適化
3. **k スイープ**：(k \in {1,2,3,5}) で品質/コストトレードオフを確認
4. **正規化の有無**：per-token 正規化・文長補正（z-score/robust）を比較
5. **アブレーション**：先頭側 (k) 項の寄与、(C^{(k)}) のみ使用の近似との性能差

---

## 9. 計算量まとめ

* 前計算（LM 呼び出し）：**(O(kN))**
* DP：**(O(N^2))**（各区間評価 O(1)）
* 全体：実運用では (N \lesssim 400) 程度なら余裕。長文多量なら二段階化（候補境界を一次元DPで絞り→再採点）も可。

---

## 10. 注記（よくハマる点）

* 改行や区切りトークン差で NLL が大きく揺れる → **整形を仕様で固定**
* モデル切替でスケールが変わる → **(\alpha) 再学習必須**
* 文分割差 → **同一ルールで凍結**
* 極端に短い/長い文 → **長さ正規化**で安定

---

この設計で「以前の coherence gain 目的」を保持しながら、「文脈は最大 k 文まで」に制約して **O(kN) 前計算 + O(N²) DP** を実現できます。品質優先ならこの版、速度優先なら先に一次元の近似で候補境界を抽出してから本 DP で確定、という二段階運用が扱いやすいです。
