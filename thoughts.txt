いいね、じゃあこの **「coherence gain ベース Viterbi チャンキング」** を、そのままエンジニアに渡せる **DP仕様書** にしてまとめるね。

---

# =========================================================

# 文チャンク分割アルゴリズム 仕様書

## （Coherence Gain × NLL × Viterbi DP）

# =========================================================

## 1. 目的

入力テキストを文単位に分割し、
**「文をバラバラに読むより、チャンクとして読む方がどれだけ言語モデルにとって説明しやすくなるか」**
（coherence gain）を最大化するように、
最適なチャンク境界列を動的計画法（Viterbi 型 DP）で決定する。

### 直感

* 文を単独で読むとそこそこ難しい
* しかし、あるまとまりとして読むと「流れが繋がって」一気に予測しやすくなる
* その「**予測しやすさの改善量**」が大きい塊を1チャンクとみなす

---

## 2. 前提・記法

* 文分割済みのテキスト：

[
S = {S_1, S_2, \dots, S_N}
]

* 文 (S_i) のトークン列：
  [
  S_i = (w^{(i)}*1, \dots, w^{(i)}*{T_i})
  ]

* チャンク (C)：連続した文区間として定義
  [
  C = S_{s..t} = {S_s, S_{s+1}, \dots, S_t}
  ]

* チャンク列：
  [
  {C_1, C_2, \dots, C_K}, \quad C_0 = \text{BOS（開始記号扱い）}
  ]

* 使用する LM は **token-level logprob を返せるモデル**（オープンモデル想定）。

---

## 3. スコア設計：Coherence Gain

### 3.1 文ごとのベースコスト（単独読み）

各文 (S_i) を **コンテキストなし**（もしくは固定の汎用プロンプトのみ）で LM に入力し、
トークンごとの負の log-likelihood を計算：

[
\text{NLL}*{\text{base}}(S_i)
= - \sum*{t=1}^{T_i} \log P(w^{(i)}*t \mid w^{(i)}*{<t}, \text{none})
]

これを「その文を単独で読むときのコスト」とする。

実装では：

```python
base_cost[i] = NLL_base(S_i)  # スカラー値
```

を前計算してキャッシュしておく。

---

### 3.2 チャンクとして読んだときの NLL

チャンク (C = S_{s..t}) を **1つのまとまりとして読む**ときの NLL を定義する。

#### コンテキストの扱い

* チャンク (C_i) の NLL は **直前チャンク (C_{i-1})** をコンテキストにして評価する：

[
\text{NLL}*{\text{chunk}}(C_i \mid C*{i-1})
= - \sum_{w \in C_i} \log P(w \mid C_{i-1} ;\text{の全文＋} C_i 内の preceding tokens)
]

* 最初のチャンク (C_1) については `context = BOS` とし、
  (\text{NLL}_{\text{chunk}}(C_1 \mid C_0)) を計算する。

実装上は：

```python
chunk_nll(C, context_chunk) -> float
```

という関数で、
`context_chunk` のテキスト＋`C` のテキストを一続きで LM に入れ、
`C` に対応するトークン部分だけの NLL を集計する。

---

### 3.3 チャンクの Coherence Gain

チャンク (C = S_{s..t}) の **coherence gain** を以下で定義する：

[
G(C \mid \text{context})
= \underbrace{\sum_{i=s}^t \text{NLL}*{\text{base}}(S_i)}*{\text{単独で読んだ場合のコスト総和}}
;-;
\underbrace{\text{NLL}*{\text{chunk}}(C \mid \text{context})}*{\text{チャンクとして読んだコスト}}
]

* 大きいほど「この塊として読むことで LM が楽になる」＝良チャンク
* 小さい or マイナスなら
  「まとめても得ではない／むしろ不自然」

DP で使う **チャンクスコア** を

[
F(C_i, C_{i-1}) = G(C_i \mid C_{i-1})
]

とする。

※ 追加で細かい正則化（長さ上限など）を入れたい場合は、
　`F` にそのペナルティ項を足し引きする。

---

## 4. 目的関数と最適化

チャンク列 ({C_1, \dots, C_K}) に対して、
総 gain を最大化する：

[
\text{TotalGain} =
\sum_{i=1}^{K} F(C_i, C_{i-1})
==============================

\sum_{i=1}^K G(C_i \mid C_{i-1})
]

DP はこの **TotalGain** を最大化する segmentation を求める。

---

## 5. DP（Viterbi）仕様

### 5.1 状態とテーブル

* 文インデックス：1〜N

DP テーブル：

* `DP_score[j]`

  * 文 1〜j を最適にチャンクしたときの **最大 TotalGain**
* `prev[j]`

  * 上記のとき、最後のチャンクの開始位置 s（s ≤ j）

**注意**：
`DP_score[j]` は「文1〜jに対する最大の総和スコア」であり、
平均などの正規化は行わない（gain は加法的なのでそのままで良い）。

---

### 5.2 初期化

```python
DP_score[0] = 0.0       # 文が0個のとき gain = 0
prev[0] = -1            # ダミー
```

---

### 5.3 遷移（前向き計算）

文 j を終端とするチャンク候補を考える：

* チャンク候補：
  [
  C = S_{s..j} \quad (1 \leq s \leq j)
  ]

* コンテキストチャンク：

  * s = 1 → context = BOS → (C_0)
  * s > 1 → context = 「文 1..(s-1) を最適に分割したときの最後のチャンク」
    → 実装では、復元用にチャンク境界を別途管理するか、
    あるいは簡略化として **context = S_{s-1}**（直前文のみ）としてもよい（設計選択）。

**基本形（フル文脈チャンクを context にする版）**

擬似コードイメージ：

```python
for j in range(1, N+1):
    DP_score[j] = -inf
    prev[j] = None

    for s in range(1, j+1):
        C = (s, j)

        # base NLL の総和は前計算から O(1)で取れるよう prefix-sum を使うことを推奨
        base_sum = sum_base_cost[s..j]  # = Σ_i NLL_base(S_i)

        # context の決定
        if s == 1:
            context = BOS
        else:
            # 文1..(s-1)の最適チャンク列の「最後のチャンク」を復元必要
            # 実装都合で簡略化するなら context = S_{s-1} にしてOK
            context = last_chunk_for_prefix(s-1)

        nll_chunk = NLL_chunk(C, context)

        gain = base_sum - nll_chunk  # = G(C | context)

        score = DP_score[s-1] + gain

        if score > DP_score[j]:
            DP_score[j] = score
            prev[j] = s
```

### 実装簡略版（context を単に「直前文」だけにする）

`context = S_{s-1}` に固定すると、
文1..(s-1)全体のチャンク構造を復元せずに済む。

この場合：

* gain は「直前文を見てこの塊を読む」場合の改善量
* 厳密なチャンク文脈ではないが、計算・実装はかなり楽になる

---

### 5.4 後ろ向き復元

最適な `prev` からチャンク列を復元：

```python
chunks = []
j = N
while j > 0:
    s = prev[j]
    chunks.append((s, j))
    j = s - 1

chunks.reverse()
# chunks: [(s1, e1), (s2, e2), ...]
```

これが最終的なチャンク境界列。

---

## 6. 計算量と実装の工夫

### 6.1 ナイーブ計算量

* s, j の 2重ループ → O(N²) 個のチャンク候補
* 各チャンクについて `NLL_chunk` を素直に LM 呼び出しすると → 終了

なので、実用上は以下の工夫が必要。

### 6.2 実用的な高速化の方向性

1. **base NLL の prefix-sum**

   [
   \text{base_prefix}[j] = \sum_{i=1}^{j} \text{NLL}_{\text{base}}(S_i)
   ]

   としておけば、

   [
   \sum_{i=s}^j \text{NLL}_{\text{base}}(S_i)
   = \text{base_prefix}[j] - \text{base_prefix}[s-1]
   ]

   を O(1) で取れる。

2. **チャンク NLL の前計算 or 近似**

   * 制限付きウィンドウ：
     `max_chunk_len` 文数 or トークン数で s の範囲を絞る
     （例：チャンクは最大 512 トークンなど）

   * 文列全体に対して一度 LM を流し、
     各 prefix の累積 logprob から
     チャンクごとの NLL を prefix 差分で計算
     （context を BOS または固定短コンテキストにする版）

   * context を「直前チャンク」ではなく「直前文」だけにすると、
     文ペア or 文列長 M までの NLL を事前計算しやすい。

具体的な高速化方法は実装環境依存なので、
本仕様では「NLL_chunk をなんらかの形で事前／効率的に計算可能にすること」とだけ明記する。

---

## 7. 拡張オプション（任意）

本仕様のコアは **G = base_sum − chunkNLL** だが、
必要であれば以下を加える：

1. **チャンク長のソフト制約**

   * チャンクが長すぎると読みづらい場合：

   [
   F'(C_i, C_{i-1}) = G(C_i \mid C_{i-1})

   * \gamma \cdot \max(0, \text{len_tok}(C_i) - L_{\max})^2
     ]

   ただし、「一貫性で切る」ことを最優先するなら
   (\gamma) は小さく設定するか、あえて 0 のままでも良い。

2. **平均化バージョン**

   * もし「チャンク数によらずスケールを揃えたい」場合には、
     最後に
     (\text{TotalGain} / K) を見てもよいが、
     DP の更新自体は **和** で問題ない。

---

## 8. 出力仕様

* 出力：チャンク境界のリスト

```python
[
  (s1, e1),  # 1つ目のチャンクは文 s1〜e1
  (s2, e2),  # 2つ目のチャンク
  ...
]
```

* 追加情報（任意）：

  * 各チャンクの `G(C_i | C_{i-1})`
  * チャンク内文の NLL / base NLL

---

## 9. まとめ（仕様の一文要約）

> 各文の「単独NLL」と、
> チャンクとして読んだときの「チャンクNLL」の差（coherence gain）を
> チャンクスコアとし、
> その総和が最大になるように文列を Viterbi DP で分割する。

---

これをそのまま Notion か設計ドキュメントにコピペして、
下に「NLL の具体的計算方法（どの LM・どのプロンプトか）」だけ別セクションで決めれば、
実装者は迷わずコードを書けるはず。
