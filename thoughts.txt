いいテーマきた。
じゃあ「**k-gram 文脈を使った効率的な coherence chunking**」の仕様書をちゃんと書こう。

---

# 文チャンク分割アルゴリズム仕様書

## （k-gram LM Coherence × Viterbi 型 DP）

---

## 1. 目的

文列 (S_1, \dots, S_N) を **チャンク**に分割する。
各チャンクは、

> 「そのチャンク内で、言語モデルにとって予測しやすい（coherent）」

ようになっていて、かつ

> 「チャンク間には、切るだけの理由（文脈の断絶）がある」

ような分割を選びたい。

ここでの特徴は：

* **k-gram 文脈**
  → 直前 1 文だけでなく、**直前 k 文まで**を文脈として NLL を評価する
* **効率性**
  → 文書長が大きくても、
  計算量を **O(kN)** クラスに抑える

---

## 2. 設定と記法

### 2.1 文列とチャンク

* 入力テキストは文分割済みとする：

[
S_1, S_2, \dots, S_N
]

* チャンク (C) は連続区間：

[
C = S_{s..t} = {S_s, S_{s+1}, \dots, S_t}
]

* チャンク列：

[
\mathcal{C} = {C_1, C_2, \dots, C_K}, \quad C_0 = \text{BOS（仮想開始）}
]

### 2.2 使用するモデル

* トークンレベルの log-likelihood（NLL）が取れる言語モデル（LM）
* 文レベルの NLL を以下のように定義：

[
\text{NLL}(S \mid \text{context})
= - \sum_{w \in S} \log P(w \mid \text{context と S 内 preceding tokens})
]

---

## 3. k-gram 文脈と局所 gain の定義

### 3.1 文単体 NLL（ベースライン）

各文 (S_i) を「文脈なし」で読んだ NLL：

[
B_i = \text{NLL}(S_i \mid \text{none})
]

→ これは後で「文脈あり NLL」との差分を取るための基準。

### 3.2 k-gram 文脈付き NLL

最大文脈長 (K)（k-gram の k）を固定する。

文 (S_i) に対して、
ひとつ前〜最大 K 文までの文脈で NLL を計算：

[
C_i^{(m)} =
\text{NLL}(S_i \mid S_{i-m}, S_{i-m+1}, \dots, S_{i-1})
]

* (m = 1, \dots, \min(K, i-1))
* (m = 0) のときは「文脈なし」とし、
  (C_i^{(0)} = B_i) とみなす

### 3.3 局所 coherence gain（文 i の文脈 m による改善量）

[
g_i^{(m)} = B_i - C_i^{(m)}
]

* (g_i^{(m)} > 0)：
  文脈（直前 m 文）のおかげで NLL が下がった →
  「その文脈のもとで文 i を同じチャンクに入れると得」
* (g_i^{(m)} < 0)：
  その文脈はむしろ邪魔 → 同じチャンクにすると損

---

## 4. 目的関数（スコア）の考え方

### 4.1 直感的なゴール

* チャンク内では「適切な過去 m 文」を文脈として使うことで、
  **文単体より予測しやすくなる**（gain が出る）
* チャンク間の境界を増やすことには「コスト」を課す（切りすぎ防止）

### 4.2 グローバル目的

元々の“完全版”は

[
\text{TotalScore} =
\underbrace{\sum_{i=1}^N B_i}*{\text{文単体の NLL（固定）}}
+
\underbrace{\sum*{i=1}^N \text{文脈による改善量}}_{\text{区切りで変わる}}
----------------------------------------------------------

\underbrace{\alpha \cdot (\text{境界の数})}_{\text{境界コスト}}
]

ここで (\sum B_i) は segmentation に関係なく一定値なので、
最適化の対象は

[
\text{Score} =
\sum_{i=1}^N \text{gain}_i

* \alpha \cdot \text{#boundaries}
  ]

だけでよい。

### 4.3 文 i の gain の選び方

文 i を**どれくらい前までを同じチャンクとみなすか**で、使う (m) が変わり、
gain が (g_i^{(m)}) に決まる：

* 文 i がチャンクの先頭なら
  → 「過去文が同じチャンクに存在しない」ので (m = 0)、gain = 0
* 文 i がチャンク内の 2 個目なら
  → 最大 (m=1)（直前1文）まで
* 文 i がチャンク内の 3 個目なら
  → 最大 (m=2)（直前2文）まで
* …
* 文 i がチャンク内の r 個目なら
  → (m \le \min(K, r-1))

DP では、
「いまの文 i について、**同じチャンクの中で過去何文ぶんの文脈を使うか**」
を状態として持つ。

---

## 5. DP（Viterbi）設計

### 5.1 状態定義

最大文脈長を (K) とする。

* i: いま注目している文インデックス（1..N）
* m:

  * 「**現在のチャンク内で、i の直前までに存在する文数**」
  * かつ「文脈に使える最大文数」
  * (m \in {0, 1, 2, \dots, K})

つまり状態 ((i, m)) の意味：

> 文1..i まで見終わった時点で、
> 文 i は現在のチャンクの最後の文であり、
> そのチャンク内には直前に m 文が連なっている（＝チャンク長は m+1 以上）状態

これに対する DP テーブル：

* `DP[i][m]` :
  文1..iまでを処理し、
  状態 m（上記）で終わるときの **Score の最大値**

別途、復元用に

* `prev_i[i][m]` : 直前の i-1 の m’
* `prev_m[i][m]` : どの m’ から来たか
* `boundary[i][m]` : ここで境界を置いたかどうか

などを保持。

---

### 5.2 初期化

文1はチャンク先頭から始めるので、
**gain = 0**、文脈なし m=0 でよい：

```python
for m in 0..K:
    DP[0][m] = -inf  # 0文目のダミー
DP[0][0] = 0.0

# 文1を処理
i = 1
# 文1は必ず新チャンク開始とみなす（境界コストは払わない or α/2 にしてもいいがここでは0扱い）
DP[1][0] = 0.0  # gainも0
```

---

### 5.3 遷移

文 i (2 ≤ i ≤ N) について、
次の2つの遷移を考える：

1. **チャンクをここで切って文 i を新チャンク先頭にする**
2. **チャンクを継続して文 i を既存チャンクに繋げる**

#### 5.3.1 ケース1：境界を置いて新チャンク開始

* 文 i はチャンク先頭 ⇒ 文脈 m=0 ⇒ gain = 0
* 直前 i-1 の状態は m’ が何でもよいが、境界コスト −α を払う

[
DP[i][0] = \max_{m' \in 0..K} \big( DP[i-1][m'] - \alpha \big)
]

実装上は、`best_prev = max_m' DP[i-1][m']` を毎 i で一回計算してから

```python
DP[i][0] = best_prev - alpha
boundary[i][0] = True
```

とすればよい。

#### 5.3.2 ケース2：チャンクを継続（同じチャンクに文 i を追加）

ここが k-gram の肝。

直前 i-1 の状態 m’ から文 i を追加したとき、
新しい「チャンク内の過去文数」m は

[
m = \min(K, m' + 1)
]

このとき文 i の gain として使うのは

[
g_i^{(m)} = B_i - C_i^{(m)}
]

なので、遷移は

[
DP[i][m] = \max_{m' \in 0..K ;:; \min(K, m' + 1) = m}
\big( DP[i-1][m'] + g_i^{(m)} \big)
]

ここで (\min(K, m'+1) = m) を満たす m’ は実質 1通りか2通り：

* (m < K) のとき：
  (m = m'+1) ⇒ (m' = m-1)
* (m = K) のとき：
  (m' \in {K-1, K})（K-1以上は全部 K に潰れる）

なので実装はかなり簡単になる。

擬似コード：

```python
for i in range(2, N+1):
    # まず境界を置くケース（m=0）
    best_prev = max(DP[i-1][m] for m in 0..K)
    DP[i][0] = best_prev - alpha
    boundary[i][0] = True
    prev_m[i][0] = argmax_m DP[i-1][m]

    # 次にチャンク継続
    # m = 1..K-1
    for m in range(1, K):
        prev_state = m - 1
        gain = g[i][m]    # 事前計算済みの g_i^{(m)}
        DP[i][m] = DP[i-1][prev_state] + gain
        boundary[i][m] = False
        prev_m[i][m] = prev_state

    # m = K
    # prev_state could be K-1 or K
    gain = g[i][K]
    cand1 = DP[i-1][K-1] + gain
    cand2 = DP[i-1][K] + gain
    if cand1 >= cand2:
        DP[i][K] = cand1
        prev_m[i][K] = K-1
    else:
        DP[i][K] = cand2
        prev_m[i][K] = K
    boundary[i][K] = False
```

---

### 5.4 復元

最後に、文 N で最大スコアを持つ m* を選ぶ：

```python
m_star = argmax_m DP[N][m]
i = N
chunks = []
current_end = N
current_start = None
```

後ろ向きに辿る：

```python
while i > 0:
    m = m_star
    if boundary[i][m]:
        # i がチャンク先頭
        current_start = i
        chunks.append((current_start, current_end))
        current_end = i - 1

    # 1つ前へ
    prev_m_state = prev_m[i][m]
    i -= 1
    m_star = prev_m_state

# 逆順に直す
chunks.reverse()
```

境界の扱いをもう少し丁寧にすると、

* boundary==True なら「ここから新チャンク」
* boundary==False なら前と同じチャンク

としてチャンクの始端・終端を記録すればOK。

---

## 6. 前処理：NLL と g_i^{(m)} の効率的な計算

### 6.1 文単体 NLL (B_i)

全文について：

```python
for i in 1..N:
    B[i] = NLL(S_i | none)
```

* 1文ずつ LM に投げるだけなので O(N) 回
* コンテキスト長制約にも引っかからない

### 6.2 k-gram 文脈付き NLL (C_i^{(m)})

最大 K に対して：

```python
for i in 1..N:
    for m in 1..min(K, i-1):
        context = concat(S_{i-m}, ..., S_{i-1})
        C[i][m] = NLL(S_i | context)
        g[i][m] = B[i] - C[i][m]
    g[i][0] = 0.0   # 文頭扱い
```

* LM 呼び出し回数は
  (\sum_{i=1}^N \min(K, i-1) \approx O(KN))
* K は小さい（2〜3）想定なので、**全体 O(N)** に近い

これで DP 内ではすべて O(1) で参照可能。

---

## 7. 計算量とメモリ

* 前処理（NLL 計算）：

  * 文単体：N 回
  * 文脈付き：最大 K 文脈 × N → **O(KN)** LM 呼び出し
* DP：

  * i=1..N, m=0..K のループ → **O(KN)**

最大文脈長 K を定数（例えば 2 or 3）に固定すれば、

> **時間計算量：O(N)**（定数 K を無視したオーダー）
> メモリ：O(KN)

となる。

---

## 8. あなたの問題意識との対応関係

* A→B はあまり gain がない（g_B^{(1)} 小さい）
* でも B→C, A,B→C の文脈では NLL が大きく改善する場合、

  * C に対して (g_C^{(2)}) が高くなる
  * 「同じチャンク内で直前2文を文脈に使う」状態 (m=2) が有利になる
* DP は

  * B を A と切るべきか
  * B,C を一緒のチャンクにするべきか
    を **g_B^{(1)}, g_C^{(2)}, 境界コスト α のトレードオフ**でグローバルに決める

つまり、

> 「直前文だけではなく、直前 2〜3 文までの影響を入れた上で
> どこで切るかを DP で決める」

という形で、
**あなたの “A→B は変でも AB→C で一貫する” 問題意識を
k-gram の範囲でちゃんと反映できる**仕様になっている。

---

これをそのまま設計書として使って、
あとは「どの LM を使うか（軽量モデルか、本番モデルか）」「K=2 or 3」「α の選び方」を実験で詰めれば、そのまま RAG 用の chunker として実装に落とせるはず。
